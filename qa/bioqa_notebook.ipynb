{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the Dataset and Model\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "## Loading the dataset\n",
    "from datasets import load_dataset, ClassLabel, Metric\n",
    "from evaluate import load\n",
    "dataset = load_dataset(\"pubmed_qa\", name=\"pqa_labeled\")\n",
    "\n",
    "features = dataset['train'].features.copy()\n",
    "features['final_decision'] = ClassLabel(3, [\"yes\",\"no\", \"maybe\"])\n",
    "dataset['train'] = dataset['train'].cast(features)\n",
    "dataset = dataset.rename_column('final_decision','label')\n",
    "metric: Metric = load(\"f1\")\n",
    "\n",
    "## Loading the model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrating that our dataset has no null values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ds_df = pd.DataFrame({\n",
    "    'question': dataset['train']['question'],\n",
    "    'context' : dataset['train']['context']\n",
    "})\n",
    "ds_df = ds_df.convert_dtypes(infer_objects=True)\n",
    "ds_na = ds_df[ds_df.isnull().any(axis=1)]\n",
    "ds_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_long_answer(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"long_answer\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "def preprocess_with_context(examples):\n",
    "    question = examples['question']\n",
    "    context = examples['context.contexts']\n",
    "    \n",
    "    # Combine context sentences into a single string\n",
    "    context_strs = [' '.join(context_str) for context_str in context]\n",
    "    \n",
    "    # Tokenize inputs with overlap\n",
    "    return tokenizer(\n",
    "        question,\n",
    "        context_strs,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        stride=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_reasoning_required = dataset.flatten().map(preprocess_with_context, batched=True)\n",
    "encoded_reasoning_free = dataset.map(preprocess_with_long_answer, batched=True)\n",
    "\n",
    "# encoded_dataset = encoded_reasoning_required\n",
    "encoded_dataset = encoded_reasoning_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_valid = encoded_dataset['train'].train_test_split(test_size=.5)\n",
    "\n",
    "train_test = train_valid['train'].train_test_split(test_size=.1)\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train':train_test['train'],\n",
    "    'test':train_test['test'],\n",
    "    'validation':train_valid['test']\n",
    "})\n",
    "train_test_valid_dataset = train_test_valid_dataset.remove_columns(('context.contexts', 'context.labels', 'context.meshes', 'context.reasoning_required_pred', 'context.reasoning_free_pred', 'long_answer', 'pubid', 'question'))\n",
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuning the model\n",
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "import numpy as np\n",
    "# To instantiate a `Trainer`, we will need to define two more things.\n",
    "# The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments),\n",
    "# which is a class that contains all the attributes to customize the\n",
    "# training. It requires one folder name, which will be used to save\n",
    "# the checkpoints of the model, and all other arguments are optional:\n",
    "\n",
    "metric_name = \"f1\"\n",
    "i = -2 if model_checkpoint.endswith('/') else -1\n",
    "model_name = model_checkpoint.split(\"/\")[i]\n",
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-pqa-l\",\n",
    "    evaluation_strategy = IntervalStrategy.EPOCH,\n",
    "    do_eval=True,\n",
    "    save_strategy = IntervalStrategy.EPOCH,\n",
    "    logging_strategy=IntervalStrategy.EPOCH,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Here we set the evaluation to be done at the end of each epoch, tweak the\n",
    "# learning rate, use the `batch_size` defined at the top of the script and\n",
    "# customize the number of epochs for training, as well as the weight decay.\n",
    "# Since the best model might not be the one at the end of training, we ask the\n",
    "# `Trainer` to load the best model it saved (according to `metric_name`) at the\n",
    "# end of training.\n",
    "# The last thing to define for our `Trainer` is how to compute the metrics from\n",
    "# the predictions. We need to define a function for this, which will just use\n",
    "# the `metric` we loaded earlier, the only preprocessing we have to do is to\n",
    "# take the argmax of our predicted logits\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average='micro')\n",
    "\n",
    "\n",
    "# Then we just need to pass all of this along with our datasets to the `Trainer`:\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_test_valid_dataset[\"train\"],\n",
    "    eval_dataset=train_test_valid_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# for batch in trainer.get_eval_dataloader(train_test_valid_dataset[\"validation\"]):\n",
    "    # print(batch)\n",
    "    # break\n",
    "\n",
    "# We can now finetune our model by just calling the `train` method:\n",
    "trainer.train()\n",
    "\n",
    "# We can check with the `evaluate` method that our `Trainer` did\n",
    "# reload the best model properly (if it was not the last one):\n",
    "# trainer.evaluate()\n",
    "\n",
    "# Testing and printing results\n",
    "print(trainer.predict(test_dataset=train_test_valid_dataset[\"valid\"]).metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
